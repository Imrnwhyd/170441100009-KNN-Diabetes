{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Pengertian K-Nearest Neighbor K-nearest neighbors atau knn adalah algoritma yang berfungsi untuk melakukan klasifikasi suatu data berdasarkan data pembelajaran (*train data sets*), yang diambil dari k tetangga terdekatnya (*nearest neighbors*). Dengan k merupakan banyaknya tetangga terdekat. Tujuan dari algoritma ini adalah mengklasifikasi objek baru berdasakan atribut dan sampel latih. pengklasifikasian tidak menggunakan model apapun untuk dicocokkan dan hanya berdasarkan pada memori. Diberikan titik uji, akan ditemukan sejumlah K objek (titik training) yang paling dekat dengan titik uji. Klasifikasi menggunakan voting terbanyak di antara klasifikasi dari K objek. Algoritma K-NN menggunakan klasifikasi ketetanggaan sebagai nilai prediksi dari sample uji yang baru. Dekat atau jauhnya tetangga biasanya dihitung berdasarkan jarak Eucledian. Nilai k yang terbaik untuk algoritma ini tergantung pada data. Secara umum, nilai k yang tinggi akan mengurangi efek noise pada klasifikasi, tetapi membuat batasan antara setiap klasifikasi menjadi lebih kabur. Nilai k yang bagus dapat dipilih dengan optimasi parameter, misalnya dengan menggunakan cross-validation. Kasus khusus di mana klasifikasi diprediksikan berdasarkan data pembelajaran yang paling dekat (dengan kata lain, k = 1) disebut algoritma k-nearest neighbor. Ketepatan algoritma k-NN ini sangat dipengaruhi oleh ada atau tidaknya fitur-fitur yang tidak relevan, atau jika bobot fitur tersebut tidak setara dengan relevansinya terhadap klasifikasi. Riset terhadap algoritma ini sebagian besar membahas bagaimana memilih dan memberi bobot terhadap fitur agar performa klasifikasi menjadi lebih baik. Sesuai dengan prinsip kerja K-Nearest Neighbor yaitu mencari jarak terdekat antara data yang akan dievaluasi dengan k tetangga(neighbor) terdekatnya dalam data pelatihan. Persamaan dibawah ini menunjukkan rumus perhitungan untuk mencari jarak terdekat dengan d adalah jarak dan p adalah dimensi data(Agusta, 2007): Dengan keterangan : \ud835\udc651 : sampel data \ud835\udc652 : data uji i : variabel data d : jarak p : dimensi data","title":"Pendahuluan"},{"location":"#pengertian-k-nearest-neighbor","text":"K-nearest neighbors atau knn adalah algoritma yang berfungsi untuk melakukan klasifikasi suatu data berdasarkan data pembelajaran (*train data sets*), yang diambil dari k tetangga terdekatnya (*nearest neighbors*). Dengan k merupakan banyaknya tetangga terdekat. Tujuan dari algoritma ini adalah mengklasifikasi objek baru berdasakan atribut dan sampel latih. pengklasifikasian tidak menggunakan model apapun untuk dicocokkan dan hanya berdasarkan pada memori. Diberikan titik uji, akan ditemukan sejumlah K objek (titik training) yang paling dekat dengan titik uji. Klasifikasi menggunakan voting terbanyak di antara klasifikasi dari K objek. Algoritma K-NN menggunakan klasifikasi ketetanggaan sebagai nilai prediksi dari sample uji yang baru. Dekat atau jauhnya tetangga biasanya dihitung berdasarkan jarak Eucledian. Nilai k yang terbaik untuk algoritma ini tergantung pada data. Secara umum, nilai k yang tinggi akan mengurangi efek noise pada klasifikasi, tetapi membuat batasan antara setiap klasifikasi menjadi lebih kabur. Nilai k yang bagus dapat dipilih dengan optimasi parameter, misalnya dengan menggunakan cross-validation. Kasus khusus di mana klasifikasi diprediksikan berdasarkan data pembelajaran yang paling dekat (dengan kata lain, k = 1) disebut algoritma k-nearest neighbor. Ketepatan algoritma k-NN ini sangat dipengaruhi oleh ada atau tidaknya fitur-fitur yang tidak relevan, atau jika bobot fitur tersebut tidak setara dengan relevansinya terhadap klasifikasi. Riset terhadap algoritma ini sebagian besar membahas bagaimana memilih dan memberi bobot terhadap fitur agar performa klasifikasi menjadi lebih baik. Sesuai dengan prinsip kerja K-Nearest Neighbor yaitu mencari jarak terdekat antara data yang akan dievaluasi dengan k tetangga(neighbor) terdekatnya dalam data pelatihan. Persamaan dibawah ini menunjukkan rumus perhitungan untuk mencari jarak terdekat dengan d adalah jarak dan p adalah dimensi data(Agusta, 2007): Dengan keterangan : \ud835\udc651 : sampel data \ud835\udc652 : data uji i : variabel data d : jarak p : dimensi data","title":"Pengertian K-Nearest Neighbor"},{"location":"algoritma/","text":"Algoritma Algoritma metode KNN sangatlah sederhana, bekerja berdasarkan jarak terpendek dari query instance ke training sample untuk menentukan KNN-nya. Training sample diproyeksikan ke ruang berdimensi banyak, dimana masing-masing dimensi merepresentasikan fitur dari data. Ruang ini dibagi menjadi bagian-bagian berdasarkan klasifikasi training sample. Sebuah titik pada ruang ini ditandai kelac c jika kelas c merupakan klasifikasi yang paling banyak ditemui pada k buah tetangga terdekat dari titik tersebut. Dekat atau jauhnya tetangga biasanya dihitung berdasarkan Euclidean Distance. Tentukan parameter K (jumlah tetangga paling dekat). Hitung kuadrat jarak euclid masing-masing objek terhadap data sample yang diberikan. Urutkan objek-objek kedalam kelompok yang memiliki jarak terkecil. Kumpulkan kategori Y (klasifikasi nearest neighbor). Dengan kategori nearest neighbor yang paling banyak, maka dapt diprediksikan nilai query instance yang telah dihitung. Contoh Kasus (Hitung Manual) Data didapatkan dari kuesioner dengan obyek pengujian berupa dua atribut (daya tahan keasaman dan kekuatan) untuk mengklasifikasikan apakah sebuah kertas tissue tergolong bagus atau jelek. Berikut ini contoh datanya: X1 = Daya tahan keasaman (detik) X2 = Kekuatan (kg/m 2 ) Klasifikasi 7 7 Jelek 7 4 Jelek 3 4 Bagus 1 4 Bagus Sebuah pabrik memproduksi kertas tissue baru yang memiliki X1 = 3 dan X2 = 7. Kita gunakan algoritma KNN untuk melakukan prediksi termasuk klasifikasi apa (bagus atau jelek) kertas tissue yang baru ini. Jawaban Tentukan parameter K = jumlah banyaknya tetangga terdekat. Misal K=3. Hitung jarak antara data baru dan semua data yang ada di data training. Misal digunakan square distance dari jarak antara data baru dengan semua data yang ada di data training. X1 = Daya tahan keasaman (detik) X2 = Kekuatan (kg/m2) Square Distance ke data baru (3,7) 7 7 (7-3) 2 +(7-7) 2 = 16 7 4 (7-3) 2 +(4-7) 2 = 25 3 4 (3-3) 2 +(4-7) 2 = 9 1 4 (1-3) 2 +(4-7) 2 = 13 Urutkan jarak tersebut dan tentukan tetangga mana yang terdekat berdasarkan jarak minimum ke-K. X1 = Daya tahan keasaman (detik) X2 = Kekuatan (kg/m2) Square Distance ke data baru (3,7) Urutan Jarak K-3-NN? 7 7 (7-3)2+(7-7)2 = 16 3 Ya 7 4 (7-3)2+(4-7)2 = 25 4 Tidak 3 4 (3-3)2+(4-7)2 = 9 1 Ya 1 4 (1-3)2+(4-7)2 = 13 2 Ya Tentukan kategori dari tetangga terdekat. Perhatikan pada baris kedua pada kolom terakhir: katagori dari tetangga terdekat (Y) tidak termasuk karena ranking dari data ini lebih dari 3 (=K). X1 = Daya tahan keasaman (detik) X2 = Kekuatan (kg/m2) Square Distance ke data baru (3,7) Urutan Jarak K-3-NN? Kategory K-NN 7 7 (7-3)2+(7-7)2 = 16 3 Ya Jelek 7 4 (7-3)2+(4-7)2 = 25 4 Tidak - 3 4 (3-3)2+(4-7)2 = 9 1 Ya Bagus 1 4 (1-3)2+(4-7)2 = 13 2 Ya Bagus Gunakan kategori mayoritas yang sederhana dari tetangga yang terdekat tersebut sebagai nilai prediksi dari data yang baru. Kita punya 2 kategori Bagus dan 1 kategori Jelek, karena 2>1 maka kita simpulkan bahwa kertas tissue baru tadi yang memiliki X1 = 3 dan X2 = 7 termasuk dalam kategori Bagus. Itulah contoh penerapan k-nn pada sebuah kasus kusioner terhadap kertas tissue.","title":"Algoritma"},{"location":"algoritma/#algoritma","text":"Algoritma metode KNN sangatlah sederhana, bekerja berdasarkan jarak terpendek dari query instance ke training sample untuk menentukan KNN-nya. Training sample diproyeksikan ke ruang berdimensi banyak, dimana masing-masing dimensi merepresentasikan fitur dari data. Ruang ini dibagi menjadi bagian-bagian berdasarkan klasifikasi training sample. Sebuah titik pada ruang ini ditandai kelac c jika kelas c merupakan klasifikasi yang paling banyak ditemui pada k buah tetangga terdekat dari titik tersebut. Dekat atau jauhnya tetangga biasanya dihitung berdasarkan Euclidean Distance. Tentukan parameter K (jumlah tetangga paling dekat). Hitung kuadrat jarak euclid masing-masing objek terhadap data sample yang diberikan. Urutkan objek-objek kedalam kelompok yang memiliki jarak terkecil. Kumpulkan kategori Y (klasifikasi nearest neighbor). Dengan kategori nearest neighbor yang paling banyak, maka dapt diprediksikan nilai query instance yang telah dihitung.","title":"Algoritma"},{"location":"algoritma/#contoh-kasus-hitung-manual","text":"Data didapatkan dari kuesioner dengan obyek pengujian berupa dua atribut (daya tahan keasaman dan kekuatan) untuk mengklasifikasikan apakah sebuah kertas tissue tergolong bagus atau jelek. Berikut ini contoh datanya: X1 = Daya tahan keasaman (detik) X2 = Kekuatan (kg/m 2 ) Klasifikasi 7 7 Jelek 7 4 Jelek 3 4 Bagus 1 4 Bagus Sebuah pabrik memproduksi kertas tissue baru yang memiliki X1 = 3 dan X2 = 7. Kita gunakan algoritma KNN untuk melakukan prediksi termasuk klasifikasi apa (bagus atau jelek) kertas tissue yang baru ini. Jawaban Tentukan parameter K = jumlah banyaknya tetangga terdekat. Misal K=3. Hitung jarak antara data baru dan semua data yang ada di data training. Misal digunakan square distance dari jarak antara data baru dengan semua data yang ada di data training. X1 = Daya tahan keasaman (detik) X2 = Kekuatan (kg/m2) Square Distance ke data baru (3,7) 7 7 (7-3) 2 +(7-7) 2 = 16 7 4 (7-3) 2 +(4-7) 2 = 25 3 4 (3-3) 2 +(4-7) 2 = 9 1 4 (1-3) 2 +(4-7) 2 = 13 Urutkan jarak tersebut dan tentukan tetangga mana yang terdekat berdasarkan jarak minimum ke-K. X1 = Daya tahan keasaman (detik) X2 = Kekuatan (kg/m2) Square Distance ke data baru (3,7) Urutan Jarak K-3-NN? 7 7 (7-3)2+(7-7)2 = 16 3 Ya 7 4 (7-3)2+(4-7)2 = 25 4 Tidak 3 4 (3-3)2+(4-7)2 = 9 1 Ya 1 4 (1-3)2+(4-7)2 = 13 2 Ya Tentukan kategori dari tetangga terdekat. Perhatikan pada baris kedua pada kolom terakhir: katagori dari tetangga terdekat (Y) tidak termasuk karena ranking dari data ini lebih dari 3 (=K). X1 = Daya tahan keasaman (detik) X2 = Kekuatan (kg/m2) Square Distance ke data baru (3,7) Urutan Jarak K-3-NN? Kategory K-NN 7 7 (7-3)2+(7-7)2 = 16 3 Ya Jelek 7 4 (7-3)2+(4-7)2 = 25 4 Tidak - 3 4 (3-3)2+(4-7)2 = 9 1 Ya Bagus 1 4 (1-3)2+(4-7)2 = 13 2 Ya Bagus Gunakan kategori mayoritas yang sederhana dari tetangga yang terdekat tersebut sebagai nilai prediksi dari data yang baru. Kita punya 2 kategori Bagus dan 1 kategori Jelek, karena 2>1 maka kita simpulkan bahwa kertas tissue baru tadi yang memiliki X1 = 3 dan X2 = 7 termasuk dalam kategori Bagus. Itulah contoh penerapan k-nn pada sebuah kasus kusioner terhadap kertas tissue.","title":"Contoh Kasus (Hitung Manual)"},{"location":"implementasi/","text":"Tools Sebelum menerapkan konsep k-nearest neighbor pada studi kasus yang telah ditentukan, beberapa tools yang perlu dipersiapkan agar program yang kita rancang bisa dieksekusi dengan baik diantaranya: python 3.x (versi 3 keatas). Anaconda Navigator atau Pycharm. Untuk mempermudah kawan-kawan mendapatkan toolsnya, sillakan kawan-kawan bisa download tools-nya disini . Studi kasus pada Fruit Data adalah mengklasifikasikan beberapa jenis buah dengan 3 ketentuan inputan. Untuk contoh penulisan program dan datanya bisa mengambil kawan-kawan download disini . Berikut source code dan penjelasan untuk menyelesaikan study kasus tersebut dengan K-NN Classification. Langkah-langkah: Pertama Import beberapa library dari python seperti: pandas => memuat sebuah file ke dalam tabel virtual ala spreadsheet . numpy => untuk operasi vektor dan matriks. Fiturnya hampir sama dengan MATLAB dalam mengelola array dan array multidimensi. matpolib => untuk menyajikan visualisasi data cluster . sklearn => untuk mengimportkan library data science #import library import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import confusion_matrix from sklearn.metrics import f1_score from sklearn.metrics import accuracy_score import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline Kedua Mengimport data dari komputer dengan perintah pandas. #read data data = pd.read_csv('D:\\MATKUL SI\\Matkul UTM Semester4\\diabetes.csv') # Pastikan file data set berada dalam folder yang sama dengan file jupyter notebook Ketiga Menampilkan data. #explore data print(data.shape) data.head(15) # Menampilkan 15 baris pertama dari tabel Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome 0 6 148.0 72.0 35.0 155.0 33.6 0.627 50 1 1 1 85.0 66.0 29.0 155.0 26.6 0.351 31 0 2 8 183.0 64.0 29.0 155.0 23.3 0.672 32 1 3 1 89.0 66.0 23.0 94.0 28.1 0.167 21 0 4 0 137.0 40.0 35.0 168.0 43.1 2.288 33 1 5 5 116.0 74.0 29.0 155.0 25.6 0.201 30 0 6 3 78.0 50.0 32.0 88.0 31.0 0.248 26 1 7 10 115.0 72.0 29.0 155.0 35.3 0.134 29 0 8 2 197.0 70.0 45.0 543.0 30.5 0.158 53 1 9 8 125.0 96.0 29.0 155.0 32.0 0.232 54 1 10 4 110.0 92.0 29.0 155.0 37.6 0.191 30 0 11 10 168.0 74.0 29.0 155.0 38.0 0.537 34 1 12 10 139.0 80.0 29.0 155.0 27.1 1.441 57 0 13 1 189.0 60.0 23.0 846.0 30.1 0.398 59 1 14 5 166.0 72.0 19.0 175.0 25.8 0.587 51 1 Keempat Mengganti beberapa kolom dengan nilai 0 rata-rata zero_not_accepted = ['Glucose','BloodPressure','SkinThickness','BMI','Insulin'] # for col in zero_not_accepted: # for i in data[col]: # if i==0: # colSum = sum(data[col]) # meanCol=colSum/len(data[col]) # data[col]=meanCol for col in zero_not_accepted: data[col]= data[col].replace(0,np.NaN) mean = int(data[col].mean(skipna=True)) data[col] = data[col].replace(np.NaN,mean) Kelima Selanjutnya mengekstrak variable yang berdiri sendiri X = data.iloc[:,0:8] Keenam mengekstrak variable yang berada dalam kelompok y = data.iloc[:,8] Ketujuh explore data untuk mengatahui relasi data sns.heatmap(data.corr()) plt.figure(figsize=(25,7)) sns.countplot(x='Age',hue='Outcome',data=data,palette='Set1') Kedelapan memisahkan data menjadi data training and testing #build model & train data X = data[['Glucose','BloodPressure','Age']] y = data['Outcome'] #split data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=0) print('X_train = ', X_train.shape) print('X_test = ', X_test.shape) print('y_train = ', y_train.shape) print('y_test = ', y_test.shape) X_train = (537, 3) X_test = (231, 3) y_train = (537,) y_test = (231,) X_train.head() Glucose BloodPressure Age 580 151.0 90.0 21 418 83.0 68.0 27 764 122.0 70.0 27 363 146.0 78.0 67 757 123.0 72.0 52 y_train.head() 580 1 418 0 764 0 363 1 757 1 Name: Outcome, dtype: int64 Kesembilan menstandarkan nilai melalui scaling scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) Kesepuluh Memuat Model KNN classifier = KNeighborsClassifier(n_neighbors=5,p=2,metric='euclidean') classifier.fit(X_train,y_train) KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean', metric_params=None, n_jobs=None, n_neighbors=5, p=2, weights='uniform') Kesebelas Selanjutnya Membuat Prediksi y_pred = classifier.predict(X_test) Sedua belas Dilanjutkan evaluasi model conf_matrix = confusion_matrix(y_test,y_pred) print(conf_matrix) print(f1_score(y_test,y_pred)) [[140 17] [ 40 34]] 0.5439999999999999 Ketiga Belas Terakhir menentukan akurasi print(accuracy_score(y_test,y_pred)) Sehingga menghasilkan 0.7532467532467533 Donwload data file csvnya di link bawah ini : Diabetes.csv","title":"Implementasi (Studi Kasus)"},{"location":"implementasi/#tools","text":"Sebelum menerapkan konsep k-nearest neighbor pada studi kasus yang telah ditentukan, beberapa tools yang perlu dipersiapkan agar program yang kita rancang bisa dieksekusi dengan baik diantaranya: python 3.x (versi 3 keatas). Anaconda Navigator atau Pycharm. Untuk mempermudah kawan-kawan mendapatkan toolsnya, sillakan kawan-kawan bisa download tools-nya disini . Studi kasus pada Fruit Data adalah mengklasifikasikan beberapa jenis buah dengan 3 ketentuan inputan. Untuk contoh penulisan program dan datanya bisa mengambil kawan-kawan download disini . Berikut source code dan penjelasan untuk menyelesaikan study kasus tersebut dengan K-NN Classification.","title":"Tools"},{"location":"implementasi/#langkah-langkah","text":"","title":"Langkah-langkah:"},{"location":"implementasi/#pertama","text":"Import beberapa library dari python seperti: pandas => memuat sebuah file ke dalam tabel virtual ala spreadsheet . numpy => untuk operasi vektor dan matriks. Fiturnya hampir sama dengan MATLAB dalam mengelola array dan array multidimensi. matpolib => untuk menyajikan visualisasi data cluster . sklearn => untuk mengimportkan library data science #import library import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import confusion_matrix from sklearn.metrics import f1_score from sklearn.metrics import accuracy_score import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline","title":"Pertama"},{"location":"implementasi/#kedua","text":"Mengimport data dari komputer dengan perintah pandas. #read data data = pd.read_csv('D:\\MATKUL SI\\Matkul UTM Semester4\\diabetes.csv') # Pastikan file data set berada dalam folder yang sama dengan file jupyter notebook","title":"Kedua"},{"location":"implementasi/#ketiga","text":"Menampilkan data. #explore data print(data.shape) data.head(15) # Menampilkan 15 baris pertama dari tabel Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome 0 6 148.0 72.0 35.0 155.0 33.6 0.627 50 1 1 1 85.0 66.0 29.0 155.0 26.6 0.351 31 0 2 8 183.0 64.0 29.0 155.0 23.3 0.672 32 1 3 1 89.0 66.0 23.0 94.0 28.1 0.167 21 0 4 0 137.0 40.0 35.0 168.0 43.1 2.288 33 1 5 5 116.0 74.0 29.0 155.0 25.6 0.201 30 0 6 3 78.0 50.0 32.0 88.0 31.0 0.248 26 1 7 10 115.0 72.0 29.0 155.0 35.3 0.134 29 0 8 2 197.0 70.0 45.0 543.0 30.5 0.158 53 1 9 8 125.0 96.0 29.0 155.0 32.0 0.232 54 1 10 4 110.0 92.0 29.0 155.0 37.6 0.191 30 0 11 10 168.0 74.0 29.0 155.0 38.0 0.537 34 1 12 10 139.0 80.0 29.0 155.0 27.1 1.441 57 0 13 1 189.0 60.0 23.0 846.0 30.1 0.398 59 1 14 5 166.0 72.0 19.0 175.0 25.8 0.587 51 1","title":"Ketiga"},{"location":"implementasi/#keempat","text":"Mengganti beberapa kolom dengan nilai 0 rata-rata zero_not_accepted = ['Glucose','BloodPressure','SkinThickness','BMI','Insulin'] # for col in zero_not_accepted: # for i in data[col]: # if i==0: # colSum = sum(data[col]) # meanCol=colSum/len(data[col]) # data[col]=meanCol for col in zero_not_accepted: data[col]= data[col].replace(0,np.NaN) mean = int(data[col].mean(skipna=True)) data[col] = data[col].replace(np.NaN,mean)","title":"Keempat"},{"location":"implementasi/#kelima","text":"Selanjutnya mengekstrak variable yang berdiri sendiri X = data.iloc[:,0:8]","title":"Kelima"},{"location":"implementasi/#keenam","text":"mengekstrak variable yang berada dalam kelompok y = data.iloc[:,8]","title":"Keenam"},{"location":"implementasi/#ketujuh","text":"explore data untuk mengatahui relasi data sns.heatmap(data.corr()) plt.figure(figsize=(25,7)) sns.countplot(x='Age',hue='Outcome',data=data,palette='Set1')","title":"Ketujuh"},{"location":"implementasi/#kedelapan","text":"memisahkan data menjadi data training and testing #build model & train data X = data[['Glucose','BloodPressure','Age']] y = data['Outcome'] #split data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=0) print('X_train = ', X_train.shape) print('X_test = ', X_test.shape) print('y_train = ', y_train.shape) print('y_test = ', y_test.shape) X_train = (537, 3) X_test = (231, 3) y_train = (537,) y_test = (231,) X_train.head() Glucose BloodPressure Age 580 151.0 90.0 21 418 83.0 68.0 27 764 122.0 70.0 27 363 146.0 78.0 67 757 123.0 72.0 52 y_train.head() 580 1 418 0 764 0 363 1 757 1 Name: Outcome, dtype: int64","title":"Kedelapan"},{"location":"implementasi/#kesembilan","text":"menstandarkan nilai melalui scaling scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test)","title":"Kesembilan"},{"location":"implementasi/#kesepuluh","text":"Memuat Model KNN classifier = KNeighborsClassifier(n_neighbors=5,p=2,metric='euclidean') classifier.fit(X_train,y_train) KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean', metric_params=None, n_jobs=None, n_neighbors=5, p=2, weights='uniform')","title":"Kesepuluh"},{"location":"implementasi/#kesebelas","text":"Selanjutnya Membuat Prediksi y_pred = classifier.predict(X_test)","title":"Kesebelas"},{"location":"implementasi/#sedua-belas","text":"Dilanjutkan evaluasi model conf_matrix = confusion_matrix(y_test,y_pred) print(conf_matrix) print(f1_score(y_test,y_pred)) [[140 17] [ 40 34]] 0.5439999999999999","title":"Sedua belas"},{"location":"implementasi/#ketiga-belas","text":"Terakhir menentukan akurasi print(accuracy_score(y_test,y_pred)) Sehingga menghasilkan 0.7532467532467533 Donwload data file csvnya di link bawah ini : Diabetes.csv","title":"Ketiga Belas"},{"location":"lebihkurang/","text":"Kelebihan Lebih efektif di data training yang besar Dapat menghasilkan data yang lebih akurat Kekurangan Perlu ditentukan nilai k yang paling optimal yang menyatakan jumlah tetangga terdekat Biaya komputasi cukup tinggi karena perhitungan jarak harus dilakukan pada setiap query instance bersama-sama dengan seluruh instan dari training sample","title":"Kelebihan & Kekurangan"},{"location":"lebihkurang/#kelebihan","text":"Lebih efektif di data training yang besar Dapat menghasilkan data yang lebih akurat","title":"Kelebihan"},{"location":"lebihkurang/#kekurangan","text":"Perlu ditentukan nilai k yang paling optimal yang menyatakan jumlah tetangga terdekat Biaya komputasi cukup tinggi karena perhitungan jarak harus dilakukan pada setiap query instance bersama-sama dengan seluruh instan dari training sample","title":"Kekurangan"},{"location":"refrensi/","text":"Sumber: Gorunescu, F. 2011. Data Mining Concept Model and Techniques. Berlin: Springer. ISBN 978-3-642-19720-8. Han, Jiawei dan Kamber, Micheline. (2006), Data Mining : Concept and Techniques Second Edition, Morgan Kaufmann Publishers. Florin Gorunescu, Data Mining: Concepts, Models and Techniques, Springer, 2011. https://cahyadsn.phpindonesia.id/extra/knn.php https://www.academia.edu/31306621/MAKALAH_KNN_K-NEAREST_NEIGHBOUR_ pengertian-kekurangan-dan-kelebihan.","title":"Refrensi"},{"location":"refrensi/#sumber","text":"Gorunescu, F. 2011. Data Mining Concept Model and Techniques. Berlin: Springer. ISBN 978-3-642-19720-8. Han, Jiawei dan Kamber, Micheline. (2006), Data Mining : Concept and Techniques Second Edition, Morgan Kaufmann Publishers. Florin Gorunescu, Data Mining: Concepts, Models and Techniques, Springer, 2011. https://cahyadsn.phpindonesia.id/extra/knn.php https://www.academia.edu/31306621/MAKALAH_KNN_K-NEAREST_NEIGHBOUR_ pengertian-kekurangan-dan-kelebihan.","title":"Sumber:"}]}